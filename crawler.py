import os
import time
import random
import re
import pandas as pd
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# ================= é…ç½®åŒº =================
MAX_COMMENTS = 200  # ç›®æ ‡æŠ“å–æ•°é‡
SCROLL_PAUSE_MIN = 0.5  # æœ€å°ç­‰å¾…æ—¶é—´ (ç§’)
SCROLL_PAUSE_MAX = 0.8  # æœ€å¤§ç­‰å¾…æ—¶é—´ (ç§’)
MAX_STUCK_COUNT = 2  # è¿ç»­æ— æ•°æ®é€€å‡ºé˜ˆå€¼


# =========================================

def check_login_status(driver):
    """æ£€æµ‹æ˜¯å¦éœ€è¦ç™»å½•"""
    try:
        cookies = driver.get_cookies()
        for cookie in cookies:
            if cookie['name'] in ['_nk_', 'tracknick', 'lgc', '_l_g_']:
                return True
        if "é€€å‡º" in driver.page_source:
            return True
    except:
        pass
    return False


def is_junk_text(text):
    """ã€è¿‡æ»¤å™¨ã€‘å¼ºåŠ›è¿‡æ»¤ SKUã€ç³»ç»Ÿæ–‡æ¡ˆã€å•çº¯çš„æ—¥æœŸè¡Œã€æ— æ•ˆè¯„ä»·"""
    if not text: return True
    text = text.strip()

    if len(text) < 4: return True

    # å¦‚æœæ•´æ®µæ–‡å­—æœ¬èº«å°±æ˜¯æ—¥æœŸï¼Œå®ƒä¸æ˜¯è¯„è®ºå†…å®¹ï¼Œè¿‡æ»¤æ‰ï¼ˆä½†æˆ‘ä»¬ä¼šé€šè¿‡ä¸Šä¸‹æ–‡æå–å®ƒä½œä¸ºå…ƒæ•°æ®ï¼‰
    if len(text) < 25 and re.search(r'\d{4}[-å¹´]\d{1,2}[-æœˆ]\d{1,2}', text):
        return True

    if "âœ…" in text or text.startswith("ã€") or text.startswith("["): return True

    junk_keywords = [
        "å·²è´­", "é¢œè‰²åˆ†ç±»", "å°ºç ", "è§„æ ¼", "æ¬¾å¼",
        "æ­¤ç”¨æˆ·æ²¡æœ‰å¡«å†™", "ç³»ç»Ÿé»˜è®¤", "è¯„ä»·æ–¹æœªåŠæ—¶åšå‡ºè¯„ä»·",
        "æœªåŠæ—¶ä¸»åŠ¨è¯„ä»·", "ç³»ç»Ÿé»˜è®¤å¥½è¯„", "è‡ªåŠ¨å¥½è¯„",
        "ç”¨æˆ·è¯„ä»·", "æŸ¥çœ‹å…¨éƒ¨", "æµè§ˆé‡", "é”€é‡", "è¿½è¯„",
        "äººå·²ä¹°", "è§†é¢‘", "å›¾ç‰‡", "è¯„è®º", "å¤©çŒ«", "ç§¯åˆ†",
        "å·²å”®", "æ»¡", "å‡", "ç«‹å‡", "ä¸ºä½ å±•ç¤º", "çœŸå®è¯„ä»·",
        "å¦‚æœä¸æ»¡æ„", "é€€è´§", "è¿è´¹", "ä¸Šé—¨å–ä»¶",
        "é»˜è®¤æ’åº", "æŒ‰çƒ­åº¦", "æŒ‰æ—¶é—´", "æ¨è", "é—®å¤§å®¶", "å®è´ç»†èŠ‚",
        "æ——èˆ°åº—", "ä¸“å–åº—", "æœˆé”€", "åº“å­˜", "å‘è´§", "ä»˜æ¬¾", "æŠ˜"
    ]
    for k in junk_keywords:
        if k in text: return True

    if "Â¥" in text or "ï¿¥" in text: return True
    if re.search(r'æ»¡\d+å‡\d+', text): return True

    return False


def scroll_internal_panel(driver, element):
    """ã€æ ¸å¿ƒé»‘ç§‘æŠ€ã€‘åªæ»šåŠ¨å†…éƒ¨å®¹å™¨"""
    js_script = """
    var element = arguments[0];
    var scrollable = null;
    var parent = element.parentElement;
    for (var i = 0; i < 15; i++) {
        if (!parent) break;
        var style = window.getComputedStyle(parent);
        if ((style.overflowY === 'auto' || style.overflowY === 'scroll') && parent.scrollHeight > parent.clientHeight) {
            scrollable = parent;
            break;
        }
        parent = parent.parentElement;
    }
    if (scrollable) {
        scrollable.scrollTop = scrollable.scrollHeight;
        return true;
    } else {
        return false; 
    }
    """
    try:
        return driver.execute_script(js_script, element)
    except:
        return False


# === è¾…åŠ©å‡½æ•°ï¼šä¸Šä¸‹æ–‡æ—¥æœŸæå– ===
def extract_date_from_context(element):
    """
    å°è¯•ä»å½“å‰å…ƒç´ çš„çˆ¶çº§æˆ–ç¥–çˆ¶çº§æ–‡æœ¬ä¸­æå–æ—¥æœŸ
    è¿”å›æ ¼å¼: YYYY-MM-DD æˆ– YYYYå¹´MMæœˆDDæ—¥
    """
    date_pattern = r'(\d{4}[-å¹´]\d{1,2}[-æœˆ]\d{1,2})'

    try:
        # ç­–ç•¥1: æ‰¾çˆ¸çˆ¸ (Parent)
        # å¾ˆå¤šæ—¶å€™è¯„è®ºå†…å®¹å’Œæ—¥æœŸåœ¨åŒä¸€ä¸ªå¤§çš„ div å®¹å™¨é‡Œ
        parent = element.find_element(By.XPATH, "..")
        parent_text = parent.text
        match = re.search(date_pattern, parent_text)
        if match:
            return match.group(1)

        # ç­–ç•¥2: æ‰¾çˆ·çˆ· (Grandparent)
        # ç»“æ„è¾ƒæ·±æ—¶ä½¿ç”¨
        grandparent = element.find_element(By.XPATH, "../..")
        grand_text = grandparent.text
        match = re.search(date_pattern, grand_text)
        if match:
            return match.group(1)

    except:
        pass

    return ""  # æ²¡æ‰¾åˆ°


# === æœç´¢å‡½æ•° ===
def get_search_links(keyword, count=3):
    print(f"ğŸ” [Search] æ­£åœ¨æœç´¢: {keyword}")
    options = webdriver.ChromeOptions()
    options.add_argument("--disable-blink-features=AutomationControlled")
    user_data_dir = r"D:\Login_dataset\SeleniumUserData_Search"
    options.add_argument(f"--user-data-dir={user_data_dir}")
    options.add_argument("--start-maximized")
    prefs = {"profile.managed_default_content_settings.images": 2}
    options.add_experimental_option("prefs", prefs)

    driver = None
    links = []
    try:
        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
        driver.get(f"https://s.taobao.com/search?q={keyword}")
        try:
            WebDriverWait(driver, 5).until(
                EC.presence_of_element_located((By.XPATH, "//a[contains(@href, 'item.htm')]")))
        except:
            time.sleep(2)

        elements = driver.find_elements(By.XPATH, "//a[contains(@href, 'item.htm') and not(contains(@href, 'click'))]")
        for elem in elements:
            url = elem.get_attribute("href")
            if url and "id=" in url:
                if not url.startswith("http"): url = "https:" + url
                if url not in links: links.append(url)
            if len(links) >= count: break
    except Exception as e:
        print(f"æœç´¢å‡ºé”™: {e}")
        return []
    finally:
        if driver: driver.quit()
    return links


# === æ ¸å¿ƒçˆ¬è™« ===
def run_spider(target_url, worker_id=1):
    print(f"ğŸš€ [çº¿ç¨‹-{worker_id}] å¯åŠ¨ Chrome...")

    options = webdriver.ChromeOptions()
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument(
        "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    prefs = {
        "profile.managed_default_content_settings.images": 2,
        "profile.managed_default_content_settings.stylesheets": 2
    }
    options.add_experimental_option("prefs", prefs)
    options.page_load_strategy = 'eager'

    base_dir = r"D:\Login_dataset\SeleniumUserData"
    user_data_dir = f"{base_dir}_{worker_id}"
    if not os.path.exists(user_data_dir): os.makedirs(user_data_dir)
    options.add_argument(f"--user-data-dir={user_data_dir}")

    driver = None
    output_file = f"tmall_data_thread_{worker_id}.csv"
    product_title = "æœªçŸ¥å•†å“"

    try:
        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
        wait = WebDriverWait(driver, 8)

        print(f"[çº¿ç¨‹-{worker_id}] æ‰“å¼€ç½‘é¡µ: {target_url}")
        driver.get(target_url)

        try:
            product_title = driver.find_element(By.CSS_SELECTOR, "h1").text.strip()
        except:
            product_title = driver.title

        # è¿›å…¥è¯„è®ºåŒº
        try:
            driver.execute_script("window.scrollBy(0, 400);")
            nav_tab = wait.until(EC.element_to_be_clickable((By.XPATH, "//*[text()='ç”¨æˆ·è¯„ä»·']")))
            driver.execute_script("arguments[0].click();", nav_tab)
            time.sleep(0.5)
        except:
            pass

        try:
            view_all_btn = driver.find_element(By.XPATH, "//*[contains(text(), 'æŸ¥çœ‹å…¨éƒ¨è¯„ä»·')]")
            driver.execute_script("arguments[0].click();", view_all_btn)
            time.sleep(1)
        except:
            pass

        # === ä¼˜åŒ– XPath æŸ¥æ‰¾èŒƒå›´ ===
        # å°è¯•æ‰¾åˆ°è¯„è®ºåŒºçš„â€œæ ¹èŠ‚ç‚¹â€ï¼Œå¦‚æœæ‰¾ä¸åˆ°å°±ç”¨ driver (å…¨æ–‡æŸ¥æ‰¾)
        # è¿™æ ·å¯ä»¥é¿å…æœç´¢åˆ°åº•éƒ¨çš„æ¨èå•†å“
        root_element = driver
        try:
            # å¸¸è§çš„è¯„è®ºåŒºå®¹å™¨ ID æˆ– Class ç‰¹å¾
            # è¿™æ˜¯ä¸€ä¸ªå¯å‘å¼æŸ¥æ‰¾ï¼Œå¦‚æœæ‰¾ä¸åˆ°ç‰¹å®šçš„ï¼Œå°±å›é€€åˆ° driver
            candidates = driver.find_elements(By.XPATH, "//*[contains(@class, 'rate') or contains(@id, 'review')]")
            # æ‰¾ä¸€ä¸ªé¢ç§¯æ¯”è¾ƒå¤§çš„å®¹å™¨ï¼Œæˆ–è€…ç›´æ¥ç”¨ body
            # ç®€å•ç­–ç•¥ï¼šå¦‚æœæ‰¾åˆ°äº†æ˜ç¡®çš„å®¹å™¨å°±ç”¨å®¹å™¨ï¼Œå¦åˆ™å…¨æ–‡
            # è¿™é‡Œä¸ºäº†ç¨³å®šæ€§ï¼Œæˆ‘ä»¬è¿˜æ˜¯ä¸»è¦ä¾èµ– driverï¼Œä½†åœ¨ XPath ä¸ŠåŠ é™å®š
            pass
        except:
            pass

        # === æé€Ÿé‡‡é›†å¾ªç¯ ===
        comments = []
        seen_hashes = set()
        last_comment_count = 0
        stuck_count = 0

        for i in range(50):
            # ä¼˜åŒ–æŸ¥æ‰¾èŒƒå›´ï¼šä½¿ç”¨ driver.find_elements
            # è¿™é‡Œçš„ XPath "//div" æ˜¯å…¨æ–‡æŸ¥æ‰¾ã€‚
            # å¦‚æœæˆ‘ä»¬èƒ½å®šä½åˆ° root_elementï¼Œå¯ä»¥ç”¨ root_element.find_elements(By.XPATH, ".//div...") (æ³¨æ„å‰é¢çš„ç‚¹)
            # ä½†è€ƒè™‘åˆ°å…¼å®¹æ€§ï¼Œè¿™é‡Œæˆ‘ä»¬ä¿æŒ "//div"ï¼Œä½†åœ¨å¤„ç†æ—¶å¢åŠ æ—¥æœŸæå–

            elements = driver.find_elements(By.XPATH, "//div[string-length(text())>3]")

            for elem in elements:
                try:
                    text = elem.text
                    if len(text) < 4: continue

                    text_hash = hash(text)
                    if text_hash in seen_hashes: continue

                    if is_junk_text(text): continue

                    # === æå–æ—¥æœŸ (æ–°å¢) ===
                    # åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä¸ä»…ä»…æå–å†…å®¹ï¼Œè¿˜å»çˆ¶å…ƒç´ æ‰¾æ—¥æœŸ
                    date_str = extract_date_from_context(elem)

                    seen_hashes.add(text_hash)
                    clean_text = text.strip()

                    comments.append({
                        "content": clean_text,
                        "date": date_str,  # æ–°å¢æ—¥æœŸåˆ—
                        "source": f"Thread-{worker_id}"
                    })

                    if len(comments) >= MAX_COMMENTS:
                        raise StopIteration
                except:
                    continue

            current_count = len(comments)
            new_added = current_count - last_comment_count

            if i % 2 == 0:
                print(f"[çº¿ç¨‹-{worker_id}] è½®æ¬¡ {i + 1} | å·²é‡‡é›†: {current_count} | æ–°å¢: {new_added}")

            if elements:
                last_element = elements[-1]
                if new_added > 0:
                    stuck_count = 0
                    scrolled = scroll_internal_panel(driver, last_element)
                    if not scrolled:
                        driver.execute_script("arguments[0].scrollIntoView(false);", last_element)
                    time.sleep(random.uniform(SCROLL_PAUSE_MIN, SCROLL_PAUSE_MAX))
                else:
                    stuck_count += 1
                    print(f"[çº¿ç¨‹-{worker_id}] âš ï¸ æš‚æ— æ–°æ•°æ® ({stuck_count}/{MAX_STUCK_COUNT})")
                    if stuck_count >= MAX_STUCK_COUNT:
                        print(f"[çº¿ç¨‹-{worker_id}] ğŸ›‘ è¿ç»­æ— æ›´æ–°ï¼Œæå‰ç»“æŸã€‚")
                        break
                    driver.execute_script("arguments[0].scrollIntoView(true);", last_element)
                    time.sleep(1.5)
            else:
                driver.execute_script("window.scrollBy(0, 300);")
                time.sleep(1)

            last_comment_count = current_count

    except StopIteration:
        print(f"[çº¿ç¨‹-{worker_id}] ğŸ‰ é‡‡é›†è¾¾æ ‡ï¼Œåœæ­¢ã€‚")
    except Exception as e:
        if "no such window" in str(e):
            print(f"[çº¿ç¨‹-{worker_id}] æµè§ˆå™¨å·²å…³é—­")
        else:
            print(f"[çº¿ç¨‹-{worker_id}] å¼‚å¸¸: {e}")
            return f"Error: {e}", None
    finally:
        if driver:
            try:
                driver.quit()
            except:
                pass

        if comments:
            df = pd.DataFrame(comments)
            df.drop_duplicates(subset=['content'], inplace=True)
            # å¯¼å‡ºæ—¶åŒ…å« date åˆ—
            df.to_csv(output_file, index=False, encoding='utf-8-sig', lineterminator='\r\n')
            return output_file, product_title
        else:
            return "Error: æœªé‡‡é›†åˆ°æœ‰æ•ˆæ•°æ®", None